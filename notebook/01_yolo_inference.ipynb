{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91f97d13-c30b-4515-9bbd-ff684360103e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/prahlad-maharaj/yolo/yolov5\n"
     ]
    }
   ],
   "source": [
    "# YOLOv5 First Exercise ‚Äî Bus Image Detection\n",
    "# This notebook runs YOLOv5 on a sample bus image to demonstrate object detection.\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa0bb159-8932-443f-8791-d2901ebfcb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/prahlad-maharaj/yolo/yolov5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b8e8b34-0053-4085-a4b7-35325fc79471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/prahlad-maharaj/yolo/yolov5\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ab571b2-9c05-47d3-8f32-1aa637957b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmarks.py    \u001b[0m\u001b[01;34mdataset\u001b[0m/    \u001b[01;34mmodels\u001b[0m/         README.zh-CN.md   \u001b[01;34mutils\u001b[0m/\n",
      "CITATION.cff     data.yaml   \u001b[01;34mnotebook\u001b[0m/       requirements.txt  val.py\n",
      "\u001b[01;34mclassify\u001b[0m/        detect.py   \u001b[01;34moutputs\u001b[0m/        \u001b[01;34mruns\u001b[0m/             yolov5m.pt\n",
      "\u001b[01;34mcoco128\u001b[0m/         export.py   \u001b[01;34m__pycache__\u001b[0m/    \u001b[01;34msegment\u001b[0m/          yolov5s.pt\n",
      "CONTRIBUTING.md  hubconf.py  pyproject.toml  train.py\n",
      "\u001b[01;34mdata\u001b[0m/            LICENSE     README.md       tutorial.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d204fbe7-87f0-4a82-a2aa-4378f9e51267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmarks.py\t dataset     models\t     README.zh-CN.md   utils\n",
      "CITATION.cff\t data.yaml   notebook\t     requirements.txt  val.py\n",
      "classify\t detect.py   outputs\t     runs\t       yolov5m.pt\n",
      "coco128\t\t export.py   __pycache__     segment\t       yolov5s.pt\n",
      "CONTRIBUTING.md  hubconf.py  pyproject.toml  train.py\n",
      "data\t\t LICENSE     README.md\t     tutorial.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfb7d343-106c-45f8-aee5-80461fd6e543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5s.pt'], source=data/images/bus.jpg, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_format=0, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5 üöÄ 025ceee Python-3.11.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce GTX 1650, 3716MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "image 1/1 /home/prahlad-maharaj/yolo/yolov5/data/images/bus.jpg: 640x480 4 persons, 1 bus, 110.6ms\n",
      "Speed: 1.8ms pre-process, 110.6ms inference, 236.6ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/exp5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run YOLOv5 detection on the sample bus image\n",
    "!python detect.py --source data/images/bus.jpg --weights yolov5s.pt --conf 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48029c20-0d52-4f97-9eb4-1759d2c661b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=3, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
      "Command 'git fetch ultralytics' timed out after 5 seconds\n",
      "YOLOv5 üöÄ 025ceee Python-3.11.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce GTX 1650, 3716MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "remote: Enumerating objects: 17752, done.\u001b[K\n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      "remote: Counting objects: 100% (109/109), done.\u001b[K\n",
      "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7235389 parameters, 7235389 gradients, 16.6 GFLOPs\n",
      "\n",
      "Transferred 349/349 items from yolov5s.pt\n",
      "/home/prahlad-maharaj/yolo/yolov5/models/common.py:898: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "/home/prahlad-maharaj/yolo/yolov5/models/common.py:898: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ), 76.00 KiB | 15.00 KiB/s\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/prahlad-maharaj/yolo/yolov5/dataset/coco128/labels/train20\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/prahlad-maharaj/yolo/yolov5/dataset/coco128/labels/train2017\u001b[0m\n",
      "Receiving objects:   0% (113/17752), 84.00 KiB | 13.00 KiB/s\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.27 anchors/target, 0.994 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\n",
      "Plotting labels to runs/train/exp10/labels.jpg... 21.00 KiB/s\n",
      "/home/prahlad-maharaj/yolo/yolov5/train.py:357: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp10\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/prahlad-maharaj/yolo/yolov5/train.py\", line 987, in <module>\n",
      "    main(opt)\n",
      "  File \"/home/prahlad-maharaj/yolo/yolov5/train.py\", line 688, in main\n",
      "    train(opt.hyp, opt, device, callbacks)\n",
      "  File \"/home/prahlad-maharaj/yolo/yolov5/train.py\", line 392, in train\n",
      "    imgs = imgs.to(device, non_blocking=True).float() / 255  # uint8 to float32, 0-255 to 0.0-1.0\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacity of 3.63 GiB of which 81.31 MiB is free. Process 6626 has 2.97 GiB memory in use. Including non-PyTorch memory, this process has 154.00 MiB memory in use. Of the allocated memory 74.44 MiB is allocated by PyTorch, and 15.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Receiving objects:  67% (11956/17752), 10.83 MiB | 1.10 MiB/s  "
     ]
    }
   ],
   "source": [
    "!python train.py --img 640 --batch 16 --epochs 3 --data data.yaml --weights yolov5s.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "475a3c7b-d3a5-42f5-9a09-917e8836181c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=3, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0m‚ö†Ô∏è YOLOv5 is out of date by 2934 commits. Use 'git pull ultralytics master' or 'git clone https://github.com/ultralytics/yolov5' to update.\n",
      "YOLOv5 üöÄ 025ceee1 Python-3.11.7 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce GTX 1650, 3716MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7235389 parameters, 7235389 gradients, 16.6 GFLOPs\n",
      "\n",
      "Transferred 349/349 items from yolov5s.pt\n",
      "/home/prahlad-maharaj/yolo/yolov5/models/common.py:898: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "/home/prahlad-maharaj/yolo/yolov5/models/common.py:898: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/prahlad-maharaj/yolo/yolov5/dataset/coco128/labels/train20\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/prahlad-maharaj/yolo/yolov5/dataset/coco128/labels/train2017\u001b[0m\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.27 anchors/target, 0.994 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\n",
      "Plotting labels to runs/train/exp11/labels.jpg... \n",
      "/home/prahlad-maharaj/yolo/yolov5/train.py:357: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp11\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/prahlad-maharaj/yolo/yolov5/train.py\", line 987, in <module>\n",
      "    main(opt)\n",
      "  File \"/home/prahlad-maharaj/yolo/yolov5/train.py\", line 688, in main\n",
      "    train(opt.hyp, opt, device, callbacks)\n",
      "  File \"/home/prahlad-maharaj/yolo/yolov5/train.py\", line 392, in train\n",
      "    imgs = imgs.to(device, non_blocking=True).float() / 255  # uint8 to float32, 0-255 to 0.0-1.0\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacity of 3.63 GiB of which 92.50 MiB is free. Process 6626 has 2.97 GiB memory in use. Including non-PyTorch memory, this process has 154.00 MiB memory in use. Of the allocated memory 74.44 MiB is allocated by PyTorch, and 15.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "!python train.py --img 640 --batch 16 --epochs 3 --data data.yaml --weights yolov5s.pt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl-env)",
   "language": "python",
   "name": "dl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
